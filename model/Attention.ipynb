{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Mechanisms\n",
        "\n",
        "Implementation of Scaled Dot-Product Attention and Multi-Head Attention.\n",
        "\n",
        "In this notebook, you'll learn how attention works by implementing it step-by-step. Each function is broken down into its own cell, with practice cells where you can write your own implementation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "The core attention mechanism: `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Initialize ScaledDotProductAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention.\n",
        "    \n",
        "    Computes attention weights and applies them to values:\n",
        "        Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
        "    \n",
        "    The scaling by sqrt(d_k) prevents the dot products from becoming too large,\n",
        "    which would push softmax into regions with very small gradients.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Practice: Implement your own `__init__`\n",
        "\n",
        "Try implementing the initialization yourself! What do you need to store?\n",
        "- Hint: You need a dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your implementation here\n",
        "# class MyScaledDotProductAttention(nn.Module):\n",
        "#     def __init__(self, dropout: float = 0.1):\n",
        "#         # Your code here\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Implement Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention.\n",
        "        \n",
        "        Args:\n",
        "            query: Query tensor of shape (batch, heads, seq_q, d_k)\n",
        "            key: Key tensor of shape (batch, heads, seq_k, d_k)\n",
        "            value: Value tensor of shape (batch, heads, seq_k, d_v)\n",
        "            mask: Optional attention mask. True/1 values are MASKED (not attended to).\n",
        "                  Shape can be (batch, 1, 1, seq_k) or (batch, 1, seq_q, seq_k)\n",
        "                  \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Output tensor of shape (batch, heads, seq_q, d_v)\n",
        "                - Attention weights of shape (batch, heads, seq_q, seq_k)\n",
        "        \"\"\"\n",
        "        d_k = query.size(-1)\n",
        "        \n",
        "        # Compute attention scores: QK^T / sqrt(d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        \n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == True, float('-inf'))\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Add forward method to the class\n",
        "ScaledDotProductAttention.forward = forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Practice: Implement your own `forward`\n",
        "\n",
        "Try implementing the forward pass yourself! The steps are:\n",
        "1. Compute attention scores: `QK^T / sqrt(d_k)`\n",
        "2. Apply mask (if provided)\n",
        "3. Apply softmax to get attention weights\n",
        "4. Apply dropout\n",
        "5. Multiply attention weights by values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your implementation here\n",
        "# def my_forward(self, query, key, value, mask=None):\n",
        "#     # Your code here\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "Instead of performing a single attention function, we project Q, K, V multiple times with different learned projections, perform attention in parallel, then concatenate and project again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Initialize MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention mechanism.\n",
        "    \n",
        "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n",
        "    where head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert d_model % n_heads == 0, \\\n",
        "            f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension per head\n",
        "        \n",
        "        # Linear projections for Q, K, V\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        \n",
        "        # Output projection\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = ScaledDotProductAttention(dropout)\n",
        "        \n",
        "        # For storing attention weights\n",
        "        self.attn_weights: Optional[torch.Tensor] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Practice: Implement your own `__init__`\n",
        "\n",
        "Try implementing the initialization yourself! What components do you need?\n",
        "- Linear projections for Q, K, V\n",
        "- Output projection\n",
        "- Attention mechanism\n",
        "- Calculate `d_k` (dimension per head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your implementation here\n",
        "# class MyMultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "#         # Your code here\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Forward Pass - Linear Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Apply multi-head attention.\n",
        "        \n",
        "        Args:\n",
        "            query: Query tensor of shape (batch, seq_q, d_model)\n",
        "            key: Key tensor of shape (batch, seq_k, d_model)\n",
        "            value: Value tensor of shape (batch, seq_k, d_model)\n",
        "            mask: Optional attention mask\n",
        "            return_attention: Whether to return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Output tensor of shape (batch, seq_q, d_model)\n",
        "                - Attention weights if return_attention=True, else None\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # 1. Linear projections\n",
        "        q = self.w_q(query)\n",
        "        k = self.w_k(key)\n",
        "        v = self.w_v(value)\n",
        "        \n",
        "        # 2. Reshape to multiple heads\n",
        "        # (batch, seq, d_model) -> (batch, seq, n_heads, d_k) -> (batch, n_heads, seq, d_k)\n",
        "        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # 3. Apply attention\n",
        "        attn_output, attn_weights = self.attention(q, k, v, mask)\n",
        "        \n",
        "        # Store attention weights for visualization\n",
        "        if return_attention:\n",
        "            self.attn_weights = attn_weights\n",
        "        \n",
        "        # 4. Concatenate heads\n",
        "        # (batch, n_heads, seq_q, d_k) -> (batch, seq_q, n_heads, d_k) -> (batch, seq_q, d_model)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model\n",
        "        )\n",
        "        \n",
        "        # 5. Final linear projection\n",
        "        output = self.w_o(attn_output)\n",
        "        \n",
        "        if return_attention:\n",
        "            return output, attn_weights\n",
        "        return output, None\n",
        "\n",
        "# Add forward method to the class\n",
        "MultiHeadAttention.forward = forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Practice: Implement your own `forward`\n",
        "\n",
        "Try implementing the forward pass yourself! The steps are:\n",
        "1. Apply linear projections to Q, K, V\n",
        "2. Reshape to multiple heads: `(batch, seq, d_model) -> (batch, n_heads, seq, d_k)`\n",
        "3. Apply attention\n",
        "4. Concatenate heads back: `(batch, n_heads, seq, d_k) -> (batch, seq, d_model)`\n",
        "5. Apply final linear projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your implementation here\n",
        "# def my_forward(self, query, key, value, mask=None, return_attention=False):\n",
        "#     # Your code here\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Attention\n",
        "\n",
        "Let's test our implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test MultiHeadAttention\n",
        "mha = MultiHeadAttention(d_model=512, n_heads=8)\n",
        "x = torch.randn(2, 32, 512)  # (batch=2, seq=32, d_model=512)\n",
        "out, attn = mha(x, x, x, return_attention=True)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Attention shape: {attn.shape}\")\n",
        "print(f\"\\nâœ… Attention mechanism works!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
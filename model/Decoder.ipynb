{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Decoder\n",
        "\n",
        "Building the Decoder Block and the full Decoder Stack (GPT Style).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, List, Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer Decoder Block.\n",
        "    \n",
        "    Implements the decoder layer with:\n",
        "    - Masked self-attention (causal, for autoregressive generation)\n",
        "    - Optional cross-attention to encoder output (for encoder-decoder models)\n",
        "    - Feed-forward network\n",
        "    - Residual connections and layer normalization\n",
        "    \n",
        "    For GPT-style models, set use_cross_attention=False.\n",
        "    \n",
        "    Args:\n",
        "        d_model: Dimension of the model\n",
        "        n_heads: Number of attention heads\n",
        "        d_ff: Dimension of the feed-forward hidden layer\n",
        "        dropout: Dropout probability\n",
        "        activation: Activation function for FFN\n",
        "        pre_norm: Whether to use pre-layer normalization\n",
        "        use_cross_attention: Whether to include cross-attention layer\n",
        "        \n",
        "    Example:\n",
        "        >>> # GPT-style (decoder-only)\n",
        "        >>> block = DecoderBlock(d_model=512, n_heads=8, d_ff=2048, use_cross_attention=False)\n",
        "        >>> x = torch.randn(2, 10, 512)\n",
        "        >>> output, self_attn, _ = block(x)\n",
        "        \n",
        "        >>> # Encoder-decoder style\n",
        "        >>> block = DecoderBlock(d_model=512, n_heads=8, d_ff=2048, use_cross_attention=True)\n",
        "        >>> enc_output = torch.randn(2, 20, 512)\n",
        "        >>> output, self_attn, cross_attn = block(x, encoder_output=enc_output)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        d_ff: int,\n",
        "        dropout: float = 0.1,\n",
        "        activation: str = \"gelu\",\n",
        "        pre_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-6,\n",
        "        use_cross_attention: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.pre_norm = pre_norm\n",
        "        self.use_cross_attention = use_cross_attention\n",
        "        \n",
        "        # Masked multi-head self-attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        \n",
        "        # Cross-attention (optional, for encoder-decoder models)\n",
        "        if use_cross_attention:\n",
        "            self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "            self.norm_cross = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout, activation)\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        \n",
        "        # Dropout for residual connections\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        encoder_output: Optional[torch.Tensor] = None,\n",
        "        self_attn_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_mask: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass through the decoder block.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq, d_model)\n",
        "            encoder_output: Encoder output for cross-attention (batch, enc_seq, d_model)\n",
        "            self_attn_mask: Mask for self-attention (causal mask)\n",
        "            cross_attn_mask: Mask for cross-attention (padding mask)\n",
        "            return_attention: Whether to return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Output tensor of shape (batch, seq, d_model)\n",
        "                - Self-attention weights if return_attention=True\n",
        "                - Cross-attention weights if return_attention=True and use_cross_attention=True\n",
        "        \"\"\"\n",
        "        self_attn_weights = None\n",
        "        cross_attn_weights = None\n",
        "        \n",
        "        if self.pre_norm:\n",
        "            # Pre-Layer Normalization\n",
        "            \n",
        "            # 1. Masked Self-Attention\n",
        "            attn_out, self_attn_weights = self._self_attention_block(\n",
        "                self.norm1(x), self_attn_mask, return_attention\n",
        "            )\n",
        "            x = x + attn_out\n",
        "            \n",
        "            # 2. Cross-Attention (optional)\n",
        "            if self.use_cross_attention and encoder_output is not None:\n",
        "                cross_out, cross_attn_weights = self._cross_attention_block(\n",
        "                    self.norm_cross(x), encoder_output, cross_attn_mask, return_attention\n",
        "                )\n",
        "                x = x + cross_out\n",
        "            \n",
        "            # 3. Feed-Forward\n",
        "            x = x + self._ff_block(self.norm2(x))\n",
        "            \n",
        "        else:\n",
        "            # Post-Layer Normalization\n",
        "            \n",
        "            # 1. Masked Self-Attention\n",
        "            attn_out, self_attn_weights = self._self_attention_block(\n",
        "                x, self_attn_mask, return_attention\n",
        "            )\n",
        "            x = self.norm1(x + attn_out)\n",
        "            \n",
        "            # 2. Cross-Attention (optional)\n",
        "            if self.use_cross_attention and encoder_output is not None:\n",
        "                cross_out, cross_attn_weights = self._cross_attention_block(\n",
        "                    x, encoder_output, cross_attn_mask, return_attention\n",
        "                )\n",
        "                x = self.norm_cross(x + cross_out)\n",
        "            \n",
        "            # 3. Feed-Forward\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "            \n",
        "        return x, self_attn_weights, cross_attn_weights\n",
        "    \n",
        "    def _self_attention_block(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        return_attention: bool\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"Apply masked self-attention with dropout.\"\"\"\n",
        "        attn_output, attn_weights = self.self_attention(\n",
        "            x, x, x, mask=mask, return_attention=return_attention\n",
        "        )\n",
        "        return self.dropout(attn_output), attn_weights\n",
        "    \n",
        "    def _cross_attention_block(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        encoder_output: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        return_attention: bool\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"Apply cross-attention to encoder output with dropout.\"\"\"\n",
        "        attn_output, attn_weights = self.cross_attention(\n",
        "            query=x,\n",
        "            key=encoder_output,\n",
        "            value=encoder_output,\n",
        "            mask=mask,\n",
        "            return_attention=return_attention\n",
        "        )\n",
        "        return self.dropout(attn_output), attn_weights\n",
        "    \n",
        "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply feed-forward network with dropout.\"\"\"\n",
        "        return self.dropout(self.feed_forward(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Decoder Stack.\n",
        "    \n",
        "    Combines embedding layer with multiple decoder blocks to create\n",
        "    the complete decoder for autoregressive text generation.\n",
        "    \n",
        "    For GPT-style models, this is the entire model (no encoder).\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        d_model: Dimension of the model\n",
        "        n_heads: Number of attention heads\n",
        "        n_layers: Number of decoder blocks\n",
        "        d_ff: Dimension of the feed-forward hidden layer\n",
        "        max_seq_len: Maximum sequence length\n",
        "        dropout: Dropout probability\n",
        "        activation: Activation function for FFN\n",
        "        pre_norm: Whether to use pre-layer normalization\n",
        "        use_cross_attention: Whether decoder blocks include cross-attention\n",
        "        \n",
        "    Example:\n",
        "        >>> # GPT-style decoder (no encoder)\n",
        "        >>> decoder = Decoder(vocab_size=10000, d_model=512, n_heads=8, n_layers=6, d_ff=2048)\n",
        "        >>> tokens = torch.randint(0, 10000, (2, 50))\n",
        "        >>> logits = decoder(tokens)\n",
        "        >>> print(logits.shape)  # (2, 50, 10000)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        n_layers: int,\n",
        "        d_ff: int,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        activation: str = \"gelu\",\n",
        "        pre_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-6,\n",
        "        use_cross_attention: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # Embedding layer (token + positional)\n",
        "        self.embedding = TransformerEmbedding(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Stack of decoder blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                dropout=dropout,\n",
        "                activation=activation,\n",
        "                pre_norm=pre_norm,\n",
        "                layer_norm_eps=layer_norm_eps,\n",
        "                use_cross_attention=use_cross_attention\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer normalization (for pre-norm)\n",
        "        self.final_norm = nn.LayerNorm(d_model, eps=layer_norm_eps) if pre_norm else None\n",
        "        \n",
        "        # Output projection to vocabulary\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        # Pre-compute causal mask\n",
        "        self._register_causal_mask(max_seq_len)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "        \n",
        "    def _register_causal_mask(self, max_seq_len: int):\n",
        "        \"\"\"Pre-compute and register the causal attention mask.\"\"\"\n",
        "        # Create upper triangular mask (True = masked)\n",
        "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "        # Shape: (1, 1, max_seq_len, max_seq_len) for broadcasting\n",
        "        self.register_buffer('causal_mask', mask.unsqueeze(0).unsqueeze(0))\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights with small values for stable training.\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,\n",
        "        encoder_output: Optional[torch.Tensor] = None,\n",
        "        cross_attn_mask: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Optional[List[Tuple]]]:\n",
        "        \"\"\"\n",
        "        Decode input sequence autoregressively.\n",
        "        \n",
        "        Args:\n",
        "            tgt: Target token indices of shape (batch, seq)\n",
        "            encoder_output: Optional encoder output for cross-attention\n",
        "            cross_attn_mask: Optional mask for cross-attention\n",
        "            return_attention: Whether to return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Logits of shape (batch, seq, vocab_size)\n",
        "                - List of (self_attn, cross_attn) tuples if return_attention=True\n",
        "        \"\"\"\n",
        "        seq_len = tgt.size(1)\n",
        "        attention_weights = [] if return_attention else None\n",
        "        \n",
        "        # Get the causal mask for the current sequence length\n",
        "        causal_mask = self.causal_mask[:, :, :seq_len, :seq_len]\n",
        "        \n",
        "        # Apply embeddings\n",
        "        x = self.embedding(tgt)\n",
        "        \n",
        "        # Pass through decoder blocks\n",
        "        for layer in self.layers:\n",
        "            x, self_attn, cross_attn = layer(\n",
        "                x,\n",
        "                encoder_output=encoder_output,\n",
        "                self_attn_mask=causal_mask,\n",
        "                cross_attn_mask=cross_attn_mask,\n",
        "                return_attention=return_attention\n",
        "            )\n",
        "            if return_attention:\n",
        "                attention_weights.append((self_attn, cross_attn))\n",
        "        \n",
        "        # Apply final layer normalization\n",
        "        if self.final_norm is not None:\n",
        "            x = self.final_norm(x)\n",
        "        \n",
        "        # Project to vocabulary\n",
        "        logits = self.output_projection(x)\n",
        "        \n",
        "        return logits, attention_weights\n",
        "    \n",
        "    def get_embedding(self, tgt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get just the embeddings without passing through layers.\"\"\"\n",
        "        return self.embedding(tgt)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
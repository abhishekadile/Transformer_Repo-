{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT Model\n",
        "\n",
        "The full GPT Model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTOutput:\n",
        "    \"\"\"Output container for GPT model.\"\"\"\n",
        "    logits: torch.Tensor\n",
        "    loss: Optional[torch.Tensor] = None\n",
        "    attention_weights: Optional[list] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-style Decoder-Only Transformer for Text Generation.\n",
        "    \n",
        "    A complete language model that can:\n",
        "    - Compute next-token probabilities given a context\n",
        "    - Generate text autoregressively with various sampling strategies\n",
        "    - Be trained with cross-entropy loss on sequences\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        d_model: Dimension of the model (default: 512)\n",
        "        n_heads: Number of attention heads (default: 8)\n",
        "        n_layers: Number of decoder blocks (default: 6)\n",
        "        d_ff: Dimension of feed-forward layer (default: 2048)\n",
        "        max_seq_len: Maximum sequence length (default: 128)\n",
        "        dropout: Dropout probability (default: 0.1)\n",
        "        activation: Activation function (\"gelu\" or \"relu\")\n",
        "        pre_norm: Whether to use pre-layer normalization\n",
        "        \n",
        "    Example:\n",
        "        >>> model = GPTModel(vocab_size=5000, d_model=512, n_heads=8, n_layers=6)\n",
        "        >>> tokens = torch.randint(0, 5000, (2, 50))\n",
        "        >>> output = model(tokens)\n",
        "        >>> print(output.logits.shape)  # (2, 50, 5000)\n",
        "        \n",
        "        >>> # Generate text\n",
        "        >>> prompt = torch.tensor([[1, 2, 3]])  # Start tokens\n",
        "        >>> generated = model.generate(prompt, max_new_tokens=50)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        max_seq_len: int = 128,\n",
        "        dropout: float = 0.1,\n",
        "        activation: str = \"gelu\",\n",
        "        pre_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-6\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # The decoder is the complete model for GPT-style architecture\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            d_ff=d_ff,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "            pre_norm=pre_norm,\n",
        "            layer_norm_eps=layer_norm_eps,\n",
        "            use_cross_attention=False  # GPT-style, no encoder\n",
        "        )\n",
        "        \n",
        "        # Count parameters\n",
        "        self.n_params = sum(p.numel() for p in self.parameters())\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> GPTOutput:\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Token indices of shape (batch, seq)\n",
        "            labels: Optional target labels for computing loss (batch, seq)\n",
        "                   Typically this is input_ids shifted by one position\n",
        "            return_attention: Whether to return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            GPTOutput containing logits, optional loss, and optional attention weights\n",
        "        \"\"\"\n",
        "        # Get logits from decoder\n",
        "        logits, attention_weights = self.decoder(\n",
        "            input_ids, return_attention=return_attention\n",
        "        )\n",
        "        \n",
        "        # Compute loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Flatten for cross-entropy\n",
        "            # logits: (batch, seq, vocab) -> (batch * seq, vocab)\n",
        "            # labels: (batch, seq) -> (batch * seq,)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, self.vocab_size),\n",
        "                labels.view(-1),\n",
        "                ignore_index=-100  # Ignore padding tokens\n",
        "            )\n",
        "        \n",
        "        return GPTOutput(\n",
        "            logits=logits,\n",
        "            loss=loss,\n",
        "            attention_weights=attention_weights if return_attention else None\n",
        "        )\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        repetition_penalty: float = 1.0,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate text autoregressively.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Starting token indices of shape (batch, seq)\n",
        "            max_new_tokens: Maximum number of new tokens to generate\n",
        "            temperature: Sampling temperature (higher = more random)\n",
        "            top_k: If > 0, only sample from top k tokens\n",
        "            top_p: If < 1.0, use nucleus sampling with this threshold\n",
        "            repetition_penalty: Penalty for repeating tokens (> 1.0 = avoid repetition)\n",
        "            eos_token_id: Stop generation when this token is generated\n",
        "            pad_token_id: Padding token ID for batched generation\n",
        "            \n",
        "        Returns:\n",
        "            Generated token indices of shape (batch, seq + max_new_tokens)\n",
        "            \n",
        "        TODO: Implement KV caching for faster generation!\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        batch_size = input_ids.size(0)\n",
        "        device = input_ids.device\n",
        "        \n",
        "        # Track which sequences have finished (hit EOS)\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "        \n",
        "        # Generate tokens one at a time\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Truncate if sequence is too long\n",
        "            idx_cond = input_ids if input_ids.size(1) <= self.max_seq_len else \\\n",
        "                       input_ids[:, -self.max_seq_len:]\n",
        "            \n",
        "            # Get logits for next token\n",
        "            output = self(idx_cond)\n",
        "            logits = output.logits[:, -1, :]  # (batch, vocab)\n",
        "            \n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty != 1.0:\n",
        "                logits = self._apply_repetition_penalty(\n",
        "                    logits, input_ids, repetition_penalty\n",
        "                )\n",
        "            \n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                logits = logits / temperature\n",
        "            \n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                logits = self._top_k_filtering(logits, top_k)\n",
        "            \n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                logits = self._top_p_filtering(logits, top_p)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
        "            \n",
        "            # Handle EOS token\n",
        "            if eos_token_id is not None:\n",
        "                eos_mask = next_token.squeeze(-1) == eos_token_id\n",
        "                finished = finished | eos_mask\n",
        "                \n",
        "                # Replace with pad token for finished sequences\n",
        "                if pad_token_id is not None and finished.any():\n",
        "                    next_token[finished] = pad_token_id\n",
        "            \n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "            \n",
        "            # Stop if all sequences finished\n",
        "            if eos_token_id is not None and finished.all():\n",
        "                break\n",
        "        \n",
        "        return input_ids\n",
        "    \n",
        "    def _apply_repetition_penalty(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        penalty: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Apply repetition penalty to logits.\"\"\"\n",
        "        for i in range(input_ids.size(0)):\n",
        "            for token_id in set(input_ids[i].tolist()):\n",
        "                if logits[i, token_id] < 0:\n",
        "                    logits[i, token_id] *= penalty\n",
        "                else:\n",
        "                    logits[i, token_id] /= penalty\n",
        "        return logits\n",
        "    \n",
        "    def _top_k_filtering(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        top_k: int\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Filter logits to only keep top-k tokens.\"\"\"\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = float('-inf')\n",
        "        return logits\n",
        "    \n",
        "    def _top_p_filtering(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        top_p: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Filter logits using nucleus (top-p) sampling.\"\"\"\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        \n",
        "        # Find where cumulative probability exceeds top_p\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Keep at least one token\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        \n",
        "        # Scatter back to original order\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "            dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
        "        )\n",
        "        logits[indices_to_remove] = float('-inf')\n",
        "        return logits\n",
        "    \n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> 'GPTModel':\n",
        "        \"\"\"Create model from a Config object.\"\"\"\n",
        "        return cls(\n",
        "            vocab_size=config.model.vocab_size,\n",
        "            d_model=config.model.d_model,\n",
        "            n_heads=config.model.n_heads,\n",
        "            n_layers=config.model.n_layers,\n",
        "            d_ff=config.model.d_ff,\n",
        "            max_seq_len=config.model.max_seq_len,\n",
        "            dropout=config.model.dropout,\n",
        "            activation=config.model.activation,\n",
        "            pre_norm=config.model.pre_norm,\n",
        "            layer_norm_eps=config.model.layer_norm_eps\n",
        "        )\n",
        "    \n",
        "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        \n",
        "        Args:\n",
        "            non_embedding: If True, exclude embedding parameters\n",
        "            \n",
        "        Returns:\n",
        "            Number of parameters\n",
        "        \"\"\"\n",
        "        n_params = self.n_params\n",
        "        if non_embedding:\n",
        "            n_params -= self.decoder.embedding.token_embedding.embedding.weight.numel()\n",
        "        return n_params\n",
        "    \n",
        "    def estimate_flops(self, seq_len: int) -> int:\n",
        "        \"\"\"\n",
        "        Estimate FLOPs for a forward pass.\n",
        "        \n",
        "        This is a rough estimate useful for efficiency comparisons.\n",
        "        \"\"\"\n",
        "        # Embeddings\n",
        "        flops = 2 * seq_len * self.d_model\n",
        "        \n",
        "        # Per layer\n",
        "        n_layers = self.decoder.n_layers\n",
        "        d_model = self.d_model\n",
        "        d_ff = self.decoder.layers[0].feed_forward.linear1.out_features\n",
        "        \n",
        "        # Self-attention: 4 * d_model^2 (Q, K, V, O projections)\n",
        "        # + 2 * seq_len^2 * d_model (attention computation)\n",
        "        attn_flops = 4 * seq_len * d_model * d_model\n",
        "        attn_flops += 2 * seq_len * seq_len * d_model\n",
        "        \n",
        "        # FFN: 2 * d_model * d_ff * 2 (two linear layers)\n",
        "        ffn_flops = 4 * seq_len * d_model * d_ff\n",
        "        \n",
        "        flops += n_layers * (attn_flops + ffn_flops)\n",
        "        \n",
        "        # Output projection\n",
        "        flops += seq_len * d_model * self.vocab_size\n",
        "        \n",
        "        return flops\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
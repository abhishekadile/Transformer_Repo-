# Transformer Hackathon ğŸš€

Build your own GPT-style transformer model from scratch and compete on the leaderboard!

**ğŸ† Leaderboard:** [https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard](https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard)

## Quick Start (3 Steps)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run the hackathon pipeline
python run_hackathon.py

# 3. Follow the prompts!
```

That's it! The script will automatically:
- Download TinyStories dataset (~50K stories)
- Train your model for 45 minutes
- Evaluate performance
- Upload to the leaderboard

---

## ğŸ“ Repository Structure

```
transformer-hackathon/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ COLAB_GUIDE.md           # Google Colab instructions
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ config.py                 # Hyperparameters (MODIFY THIS!)
â”‚
â”œâ”€â”€ model/                    # ğŸ§  Transformer components
â”‚   â”œâ”€â”€ embeddings.py         # Token + positional embeddings
â”‚   â”œâ”€â”€ attention.py          # Multi-head self-attention
â”‚   â”œâ”€â”€ feedforward.py        # Feed-forward network
â”‚   â”œâ”€â”€ encoder_block.py      # Encoder layer
â”‚   â”œâ”€â”€ decoder_block.py      # Decoder layer
â”‚   â”œâ”€â”€ encoder.py            # Full encoder stack
â”‚   â”œâ”€â”€ decoder.py            # Full decoder stack
â”‚   â””â”€â”€ transformer.py        # Complete GPT model
â”‚
â”œâ”€â”€ data/                     # ğŸ“š Data handling
â”‚   â”œâ”€â”€ tokenizer.py          # Character/word tokenizer
â”‚   â””â”€â”€ dataset.py            # Dataset loading (TinyStories default)
â”‚
â”œâ”€â”€ utils/                    # ğŸ”§ Utilities
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â”œâ”€â”€ checkpoint.py         # Save/load models
â”‚   â””â”€â”€ huggingface_upload.py # Leaderboard integration
â”‚
â”œâ”€â”€ train.py                  # Training script
â”œâ”€â”€ evaluate.py               # Evaluation script
â”œâ”€â”€ generate.py               # Text generation
â””â”€â”€ run_hackathon.py          # ğŸ† Main hackathon script
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPT-Style Transformer                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Input: "The cat sat on the"                                    â”‚
â”‚              â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TOKEN EMBEDDING + POSITIONAL ENCODING                   â”‚   â”‚
â”‚  â”‚  â€¢ Convert tokens to vectors                             â”‚   â”‚
â”‚  â”‚  â€¢ Add position information                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              DECODER BLOCK (Ã—6)                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  MASKED MULTI-HEAD SELF-ATTENTION                  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Q, K, V projections                             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ 8 attention heads                               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Causal masking (can only see past)              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚              â†“ + Residual + LayerNorm                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  FEED-FORWARD NETWORK                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Linear(512 â†’ 2048)                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ GELU activation                                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Linear(2048 â†’ 512)                              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚              â†“ + Residual + LayerNorm                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OUTPUT PROJECTION                                        â”‚   â”‚
â”‚  â”‚  â€¢ Linear(512 â†’ vocab_size)                              â”‚   â”‚
â”‚  â”‚  â€¢ Softmax â†’ probability distribution                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â†“                                                   â”‚
â”‚  Output: "mat" (predicted next token)                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Default Configuration:
â€¢ d_model = 512 (embedding dimension)
â€¢ n_heads = 8 (attention heads)
â€¢ n_layers = 6 (decoder blocks)
â€¢ d_ff = 2048 (feed-forward dimension)
â€¢ max_seq_len = 128 (context window)
â€¢ vocab_size = ~65 (character-level)
â€¢ Parameters = ~10M
```

---

## ğŸ† Hackathon Rules

### Competition Categories

1. **ğŸ¥‡ Best Model Performance**
   - Lowest perplexity wins
   - Primary ranking metric

2. **âš¡ Most Efficient Training**
   - Highest tokens/second
   - Speed matters!

3. **ğŸ“ Best Generation Quality**
   - Highest Distinct-2 score
   - Text should be diverse and coherent

4. **ğŸ¨ Most Creative Optimization**
   - Judged by organizers
   - Document your changes!

### Rules

- Training time: **Exactly 45 minutes**
- Hardware: Use whatever you have (GPU recommended)
- Code: Modify anything except timing enforcement
- Collaboration: Team up to 4 people

---

## ğŸ”§ Optimization Ideas

Here are proven techniques to improve your model:

### Easy Wins ğŸŸ¢

```python
# config.py
# 1. Enable mixed precision (2x speedup on modern GPUs!)
config.training.use_mixed_precision = True

# 2. Increase batch size if memory allows
config.training.batch_size = 32

# 3. Try different learning rates
config.training.learning_rate = 1e-3  # or 5e-4
```

### Medium Difficulty ğŸŸ¡

```python
# 1. Gradient accumulation for larger effective batch size
config.training.gradient_accumulation_steps = 4

# 2. Bigger model (if GPU memory allows)
config.model.d_model = 768
config.model.n_layers = 8

# 3. Better learning rate schedule
config.training.lr_scheduler = "cosine"
config.training.warmup_ratio = 0.1
```

### Advanced ğŸ”´

1. **Flash Attention** (in `model/attention.py`)
   ```python
   # Replace manual attention with PyTorch's optimized version
   from torch.nn.functional import scaled_dot_product_attention
   ```

2. **SwiGLU Activation** (in `model/feedforward.py`)
   ```python
   # Use SwiGLUFeedForward instead of PositionwiseFeedForward
   from model.feedforward import SwiGLUFeedForward
   ```

3. **Gradient Checkpointing** (save memory for bigger models)
   ```python
   from torch.utils.checkpoint import checkpoint
   ```

4. **Custom Optimizer** (Lion, AdaFactor, etc.)

---

## ğŸ“Š Understanding Your Metrics

| Metric | What It Measures | Good Value |
|--------|-----------------|------------|
| **Perplexity** | Model uncertainty (lower = better) | < 20 |
| **Loss** | Cross-entropy loss | < 3.0 |
| **Tokens/sec** | Training speed | > 2000 |
| **Distinct-2** | Generation diversity | > 0.5 |

---

## ğŸ› Troubleshooting

### Common Issues

**"CUDA out of memory"**
```python
# Reduce batch size
config.training.batch_size = 8

# Or enable gradient checkpointing
# (advanced, requires code changes)
```

**"Training is too slow"**
```python
# Enable mixed precision
config.training.use_mixed_precision = True

# Reduce model size
config.model.d_model = 256
config.model.n_layers = 4
```

**"Loss is not decreasing"**
```python
# Try lower learning rate
config.training.learning_rate = 1e-4

# Check for NaN (enable gradient clipping)
config.training.max_grad_norm = 0.5
```

**"Text generation is repetitive"**
```python
# Use higher temperature and repetition penalty
config.generation.temperature = 1.0
config.generation.repetition_penalty = 1.2
```

---

## ğŸ’» Running Individual Scripts

```bash
# Train only
python train.py --max-time 10  # 10 minute test run

# Train with custom settings
python train.py --batch-size 32 --lr 1e-3 --use-amp

# Evaluate a checkpoint
python evaluate.py --checkpoint checkpoints/best.pt --generate

# Generate text interactively
python generate.py

# Generate with custom settings
python generate.py --prompt "To be or not" --temperature 1.2 --max-tokens 200
```

---

## ğŸ“ˆ Leaderboard

Results are uploaded to a shared Hugging Face dataset. View the leaderboard:

```python
from utils import display_leaderboard
display_leaderboard()
```

Or check online at: [Hugging Face Leaderboard](https://huggingface.co/datasets/transformer-hackathon/leaderboard)

---

## ğŸ“ Learning Resources

- [Attention Is All You Need (Original Paper)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [Hugging Face Transformers Course](https://huggingface.co/course)

---

## ğŸ“„ License

MIT License - feel free to use, modify, and share!

---

**Good luck and have fun! ğŸš€**

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training & Generation\n",
        "\n",
        "Putting it all together to train and generate text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add reference folder to path to allow imports from reference implementation\n",
        "current_dir = os.getcwd()\n",
        "reference_dir = os.path.join(current_dir, 'reference')\n",
        "if reference_dir not in sys.path:\n",
        "    sys.path.append(reference_dir)\n",
        "\n",
        "# Import everything from reference\n",
        "try:\n",
        "    from config import Config\n",
        "    from model import GPTModel\n",
        "    from data import create_dataloaders\n",
        "    from utils.metrics import compute_perplexity\n",
        "    from torch.cuda.amp import GradScaler, autocast\n",
        "except ImportError as e:\n",
        "    print(f\"Could not import modules: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lr_scheduler(optimizer, total_steps: int, warmup_steps: int):\n",
        "    \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
        "    \n",
        "    def lr_lambda(current_step: int) -> float:\n",
        "        if current_step < warmup_steps:\n",
        "            # Linear warmup\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        else:\n",
        "            # Cosine annealing\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "            return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    \n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    val_loader,\n",
        "    device: torch.device,\n",
        "    use_amp: bool = False,\n",
        "    max_batches: Optional[int] = 200\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model on validation set.\n",
        "    \n",
        "    Args:\n",
        "        max_batches: Limit number of batches for faster evaluation (None for all)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (average loss, perplexity)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    # Progress indicator\n",
        "    print(f\"   Evaluating on {max_batches if max_batches else 'all'} batches...\", end=\"\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            if max_batches and i >= max_batches:\n",
        "                break\n",
        "                \n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            with autocast(enabled=use_amp):\n",
        "                output = model(x, labels=y)\n",
        "                loss = output.loss\n",
        "            \n",
        "            batch_tokens = y.numel()\n",
        "            total_loss += loss.item() * batch_tokens\n",
        "            total_tokens += batch_tokens\n",
        "            \n",
        "            if i % 50 == 0:\n",
        "                print(\".\", end=\"\", flush=True)\n",
        "    \n",
        "    print() # Newline\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0.0\n",
        "    perplexity = compute_perplexity(avg_loss)\n",
        "    \n",
        "    model.train()\n",
        "    return avg_loss, perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "    \n",
        "    # Set random seed\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "    \n",
        "    # Determine device\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"\ud83d\ude80 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"\u26a0\ufe0f  No GPU available, using CPU (training will be slow)\")\n",
        "    \n",
        "    # Create dataloaders\n",
        "    print(\"\\n\ud83d\udcda Loading data...\")\n",
        "    train_loader, val_loader, tokenizer = create_dataloaders(\n",
        "        data_path=args.data_path,\n",
        "        seq_len=args.seq_len,\n",
        "        batch_size=args.batch_size,\n",
        "        seed=args.seed\n",
        "    )\n",
        "    \n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    print(f\"   Vocabulary size: {vocab_size}\")\n",
        "    \n",
        "    # Create model\n",
        "    print(\"\\n\ud83c\udfd7\ufe0f  Building model...\")\n",
        "    model = GPTModel(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=args.d_model,\n",
        "        n_heads=args.n_heads,\n",
        "        n_layers=args.n_layers,\n",
        "        d_ff=args.d_model * 4,\n",
        "        max_seq_len=args.seq_len,\n",
        "        dropout=0.1\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    \n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"   Parameters: {n_params:,} ({n_params/1e6:.1f}M)\")\n",
        "    \n",
        "    # Create optimizer\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=args.lr,\n",
        "        weight_decay=0.1,\n",
        "        betas=(0.9, 0.95)\n",
        "    )\n",
        "    \n",
        "    # Estimate total steps\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    estimated_epochs = (args.max_time * 60) / (steps_per_epoch * 0.1)  # Rough estimate\n",
        "    total_steps = int(steps_per_epoch * estimated_epochs)\n",
        "    warmup_steps = int(total_steps * 0.1)\n",
        "    \n",
        "    # Create scheduler\n",
        "    scheduler = get_lr_scheduler(optimizer, total_steps, warmup_steps)\n",
        "    \n",
        "    # Mixed precision\n",
        "    scaler = GradScaler(enabled=args.use_amp)\n",
        "    if args.use_amp:\n",
        "        print(\"   Using mixed precision (FP16)\")\n",
        "    \n",
        "    # Resume from checkpoint if requested\n",
        "    start_step = 0\n",
        "    start_epoch = 0\n",
        "    if args.resume:\n",
        "        checkpoint_path = get_latest_checkpoint(args.checkpoint_dir)\n",
        "        if checkpoint_path:\n",
        "            start_epoch, start_step, _, _ = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer, scheduler, device\n",
        "            )\n",
        "        else:\n",
        "            print(\"No checkpoint found, starting from scratch\")\n",
        "    \n",
        "    # Training metrics\n",
        "    metrics = TrainingMetrics()\n",
        "    metrics.start()\n",
        "    \n",
        "    # Save tokenizer\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    tokenizer.save(os.path.join(args.checkpoint_dir, \"tokenizer.json\"))\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\n\ud83c\udfcb\ufe0f  Starting training for {args.max_time} minutes...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    max_time_seconds = args.max_time * 60\n",
        "    checkpoint_interval_seconds = args.checkpoint_interval * 60\n",
        "    last_checkpoint_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    step = start_step\n",
        "    epoch = start_epoch\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    training_start = time.time()\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            epoch += 1\n",
        "            \n",
        "            for batch_idx, (x, y) in enumerate(train_loader):\n",
        "                step_start = time.time()\n",
        "                \n",
        "                # Check time limit\n",
        "                elapsed = time.time() - training_start\n",
        "                if elapsed >= max_time_seconds:\n",
        "                    raise StopIteration(\"Time limit reached\")\n",
        "                \n",
        "                step += 1\n",
        "                \n",
        "                # Move to device\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                \n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(enabled=args.use_amp):\n",
        "                    output = model(x, labels=y)\n",
        "                    loss = output.loss / args.grad_accum_steps\n",
        "                \n",
        "                # Backward pass\n",
        "                scaler.scale(loss).backward()\n",
        "                \n",
        "                # Gradient accumulation\n",
        "                if step % args.grad_accum_steps == 0:\n",
        "                    # Gradient clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                    \n",
        "                    # Optimizer step\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "                \n",
        "                # Update metrics\n",
        "                step_time = time.time() - step_start\n",
        "                batch_tokens = x.numel()\n",
        "                metrics.update(loss.item() * args.grad_accum_steps, batch_tokens, step_time)\n",
        "                \n",
        "                # Logging\n",
        "                if step % args.log_interval == 0:\n",
        "                    remaining = max_time_seconds - elapsed\n",
        "                    _, avg_tps = metrics.get_recent_avg(args.log_interval)\n",
        "                    gpu_util = None\n",
        "                    if device.type == 'cuda':\n",
        "                        gpu_info = get_gpu_utilization()\n",
        "                        if gpu_info:\n",
        "                            gpu_util = gpu_info.get('gpu_utilization')\n",
        "                    \n",
        "                    print_progress(\n",
        "                        step=step,\n",
        "                        loss=metrics.avg_loss,\n",
        "                        tokens_per_sec=avg_tps,\n",
        "                        lr=scheduler.get_last_lr()[0],\n",
        "                        elapsed=elapsed,\n",
        "                        remaining=remaining,\n",
        "                        gpu_util=gpu_util\n",
        "                    )\n",
        "                \n",
        "                # Evaluation\n",
        "                if step % args.eval_interval == 0:\n",
        "                    print(\"\\n\")\n",
        "                    print(\"\ud83d\udcca Evaluating...\")\n",
        "                    val_loss, val_ppl = evaluate(\n",
        "                        model, val_loader, device, \n",
        "                        args.use_amp, max_batches=200\n",
        "                    )\n",
        "                    print(f\"   Validation Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")\n",
        "                    \n",
        "                    metrics.update_from_eval(val_loss)\n",
        "                    \n",
        "                    if val_loss < best_val_loss:\n",
        "                        best_val_loss = val_loss\n",
        "                        save_checkpoint(\n",
        "                            model, optimizer, scheduler,\n",
        "                            epoch, step, val_loss,\n",
        "                            metrics.get_summary(),\n",
        "                            args.__dict__,\n",
        "                            args.checkpoint_dir,\n",
        "                            filename=\"best.pt\",\n",
        "                            tokenizer=tokenizer\n",
        "                        )\n",
        "                    print()\n",
        "                \n",
        "                # Periodic checkpointing\n",
        "                current_time = time.time()\n",
        "                if current_time - last_checkpoint_time >= checkpoint_interval_seconds:\n",
        "                    print(\"\\n\ud83d\udcbe Saving checkpoint...\")\n",
        "                    save_checkpoint(\n",
        "                        model, optimizer, scheduler,\n",
        "                        epoch, step, metrics.avg_loss,\n",
        "                        metrics.get_summary(),\n",
        "                        args.__dict__,\n",
        "                        args.checkpoint_dir,\n",
        "                        tokenizer=tokenizer\n",
        "                    )\n",
        "                    cleanup_old_checkpoints(args.checkpoint_dir, keep_last_n=3)\n",
        "                    last_checkpoint_time = current_time\n",
        "                    print()\n",
        "    \n",
        "    except (StopIteration, KeyboardInterrupt) as e:\n",
        "        print(f\"\\n\\n\u23f1\ufe0f  Training stopped: {e if isinstance(e, StopIteration) else 'Interrupted'}\")\n",
        "    \n",
        "    # Final checkpoint\n",
        "    print(\"\\n\ud83d\udcbe Saving final checkpoint...\")\n",
        "    save_checkpoint(\n",
        "        model, optimizer, scheduler,\n",
        "        epoch, step, metrics.avg_loss,\n",
        "        metrics.get_summary(),\n",
        "        args.__dict__,\n",
        "        args.checkpoint_dir,\n",
        "        filename=\"final.pt\",\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    \n",
        "    # Final evaluation\n",
        "    print(\"\\n\ud83d\udcca Final evaluation...\")\n",
        "    val_loss, val_ppl = evaluate(model, val_loader, device, args.use_amp)\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(metrics)\n",
        "    print(f\"\\nFinal Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"Final Validation Perplexity: {val_ppl:.2f}\")\n",
        "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return model, tokenizer, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_progress(\n",
        "    step: int,\n",
        "    loss: float,\n",
        "    tokens_per_sec: float,\n",
        "    lr: float,\n",
        "    elapsed: float,\n",
        "    remaining: float,\n",
        "    gpu_util: Optional[float] = None\n",
        "):\n",
        "    \"\"\"Print training progress.\"\"\"\n",
        "    elapsed_str = format_time(elapsed)\n",
        "    remaining_str = format_time(remaining)\n",
        "    ppl = compute_perplexity(loss)\n",
        "    \n",
        "    gpu_str = f\" | GPU: {gpu_util:.0f}%\" if gpu_util is not None else \"\"\n",
        "    \n",
        "    print(f\"\\rStep {step:,} | Loss: {loss:.4f} | PPL: {ppl:.2f} | \"\n",
        "          f\"Tok/s: {tokens_per_sec:.0f} | LR: {lr:.2e} | \"\n",
        "          f\"Elapsed: {elapsed_str} | ETA: {remaining_str}{gpu_str}  \", end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    model: GPTModel,\n",
        "    tokenizer: CharTokenizer,\n",
        "    prompt: str,\n",
        "    max_tokens: int = 200,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9,\n",
        "    repetition_penalty: float = 1.1,\n",
        "    device: torch.device = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate text from a prompt.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained GPT model\n",
        "        tokenizer: Tokenizer for encoding/decoding\n",
        "        prompt: Text prompt to start generation\n",
        "        max_tokens: Maximum new tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_k: Top-k sampling parameter\n",
        "        top_p: Nucleus sampling parameter\n",
        "        repetition_penalty: Penalty for repeating tokens\n",
        "        device: Device to run generation on\n",
        "        \n",
        "    Returns:\n",
        "        Generated text (including prompt)\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    prompt_ids = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([prompt_ids], device=device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(output_ids[0].tolist())\n",
        "    \n",
        "    return generated_text\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
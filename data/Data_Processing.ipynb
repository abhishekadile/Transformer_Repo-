{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Processing\n",
        "\n",
        "In this notebook, we will implement the tokenizer and dataset classes to prepare our text for the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Optional, Tuple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character Tokenizer\n",
        "\n",
        "First, we build a simple character-level tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    \"\"\"\n",
        "    Character-level tokenizer.\n",
        "    \n",
        "    Simply maps each character to a unique ID. This is the simplest\n",
        "    form of tokenization and works well for small datasets.\n",
        "    \n",
        "    Args:\n",
        "        text: Optional text to build vocabulary from\n",
        "        \n",
        "    Example:\n",
        "        >>> tokenizer = CharTokenizer(\"hello world\")\n",
        "        >>> tokens = tokenizer.encode(\"hello\")\n",
        "        >>> print(tokens)  # [0, 1, 2, 2, 3]\n",
        "        >>> print(tokenizer.decode(tokens))  # \"hello\"\n",
        "    \"\"\"\n",
        "    \n",
        "    # Special tokens\n",
        "    PAD_TOKEN = \"<PAD>\"\n",
        "    UNK_TOKEN = \"<UNK>\"\n",
        "    BOS_TOKEN = \"<BOS>\"\n",
        "    EOS_TOKEN = \"<EOS>\"\n",
        "    \n",
        "    def __init__(self, text: Optional[str] = None):\n",
        "        self.char_to_id: Dict[str, int] = {}\n",
        "        self.id_to_char: Dict[int, str] = {}\n",
        "        \n",
        "        # Add special tokens first\n",
        "        self._add_special_tokens()\n",
        "        \n",
        "        if text is not None:\n",
        "            self.build_vocab(text)\n",
        "    \n",
        "    def _add_special_tokens(self):\n",
        "        \"\"\"Add special tokens to vocabulary.\"\"\"\n",
        "        special_tokens = [self.PAD_TOKEN, self.UNK_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN]\n",
        "        for token in special_tokens:\n",
        "            idx = len(self.char_to_id)\n",
        "            self.char_to_id[token] = idx\n",
        "            self.id_to_char[idx] = token\n",
        "    \n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        return self.char_to_id[self.PAD_TOKEN]\n",
        "    \n",
        "    @property\n",
        "    def unk_token_id(self) -> int:\n",
        "        return self.char_to_id[self.UNK_TOKEN]\n",
        "    \n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        return self.char_to_id[self.BOS_TOKEN]\n",
        "    \n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        return self.char_to_id[self.EOS_TOKEN]\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.char_to_id)\n",
        "    \n",
        "    def build_vocab(self, text: str):\n",
        "        \"\"\"\n",
        "        Build vocabulary from text.\n",
        "        \n",
        "        Args:\n",
        "            text: Text to extract characters from\n",
        "        \"\"\"\n",
        "        # Get unique characters\n",
        "        chars = sorted(set(text))\n",
        "        \n",
        "        # Add to vocabulary (skip if already exists)\n",
        "        for char in chars:\n",
        "            if char not in self.char_to_id:\n",
        "                idx = len(self.char_to_id)\n",
        "                self.char_to_id[char] = idx\n",
        "                self.id_to_char[idx] = char\n",
        "    \n",
        "    def encode(\n",
        "        self, \n",
        "        text: str, \n",
        "        add_bos: bool = False,\n",
        "        add_eos: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Convert text to token IDs.\n",
        "        \n",
        "        Args:\n",
        "            text: Text to encode\n",
        "            add_bos: Whether to add beginning-of-sequence token\n",
        "            add_eos: Whether to add end-of-sequence token\n",
        "            \n",
        "        Returns:\n",
        "            List of token IDs\n",
        "        \"\"\"\n",
        "        tokens = []\n",
        "        \n",
        "        if add_bos:\n",
        "            tokens.append(self.bos_token_id)\n",
        "        \n",
        "        for char in text:\n",
        "            tokens.append(self.char_to_id.get(char, self.unk_token_id))\n",
        "        \n",
        "        if add_eos:\n",
        "            tokens.append(self.eos_token_id)\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def decode(self, tokens: List[int], skip_special: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Convert token IDs back to text.\n",
        "        \n",
        "        Args:\n",
        "            tokens: List of token IDs\n",
        "            skip_special: Whether to skip special tokens in output\n",
        "            \n",
        "        Returns:\n",
        "            Decoded text\n",
        "        \"\"\"\n",
        "        special_ids = {self.pad_token_id, self.unk_token_id, \n",
        "                      self.bos_token_id, self.eos_token_id}\n",
        "        \n",
        "        chars = []\n",
        "        for token_id in tokens:\n",
        "            if skip_special and token_id in special_ids:\n",
        "                continue\n",
        "            chars.append(self.id_to_char.get(token_id, self.UNK_TOKEN))\n",
        "        \n",
        "        return \"\".join(chars)\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save tokenizer vocabulary to file.\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'char_to_id': self.char_to_id,\n",
        "                'type': 'char'\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> 'CharTokenizer':\n",
        "        \"\"\"Load tokenizer from file.\"\"\"\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        tokenizer = cls()\n",
        "        tokenizer.char_to_id = data['char_to_id']\n",
        "        tokenizer.id_to_char = {int(v): k for k, v in data['char_to_id'].items()}\n",
        "        return tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Dataset\n",
        "\n",
        "Next, we define the PyTorch Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for language modeling.\n",
        "    \n",
        "    Creates sequences of fixed length from text for training.\n",
        "    Each sample is (input_ids, target_ids) where target is input shifted by 1.\n",
        "    \n",
        "    Args:\n",
        "        text: The text to create dataset from\n",
        "        tokenizer: Tokenizer to use for encoding\n",
        "        seq_len: Sequence length for each sample\n",
        "        \n",
        "    Example:\n",
        "        >>> tokenizer = CharTokenizer(\"hello world\")\n",
        "        >>> dataset = TextDataset(\"hello world\", tokenizer, seq_len=5)\n",
        "        >>> x, y = dataset[0]\n",
        "        >>> print(x.shape, y.shape)  # torch.Size([5]), torch.Size([5])\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str,\n",
        "        tokenizer: CharTokenizer,\n",
        "        seq_len: int = 128\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "        # Encode the entire text\n",
        "        self.tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "        \n",
        "        # Calculate number of complete sequences\n",
        "        self.n_samples = max(0, len(self.tokens) - seq_len)\n",
        "        \n",
        "        print(f\"Dataset created with {len(self.tokens):,} tokens, {self.n_samples:,} samples\")\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return self.n_samples\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get a training sample.\n",
        "        \n",
        "        Args:\n",
        "            idx: Sample index\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (input_ids, target_ids) both of shape (seq_len,)\n",
        "        \"\"\"\n",
        "        # Input: tokens from idx to idx + seq_len\n",
        "        # Target: tokens from idx + 1 to idx + seq_len + 1 (shifted by 1)\n",
        "        x = self.tokens[idx : idx + self.seq_len]\n",
        "        y = self.tokens[idx + 1 : idx + self.seq_len + 1]\n",
        "        \n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "Function to download data and create dataloaders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_tinystories(data_dir: str = \"data/raw\", max_stories: int = 50000) -> str:\n",
        "    \"\"\"\n",
        "    Download the TinyStories dataset from Hugging Face.\n",
        "    \n",
        "    TinyStories is a dataset of short stories written in simple English,\n",
        "    designed for training small language models. It's much larger than\n",
        "    TinyShakespeare and produces better results.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory to save the file\n",
        "        max_stories: Maximum number of stories to use (default: 50000)\n",
        "                    Use -1 for all stories (~2.1M)\n",
        "        \n",
        "    Returns:\n",
        "        Path to the downloaded file\n",
        "    \"\"\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    file_path = os.path.join(data_dir, \"tinystories.txt\")\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Downloading TinyStories dataset...\")\n",
        "        print(\"This may take a few minutes on first run...\")\n",
        "        \n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "        except ImportError:\n",
        "            print(\"Installing datasets library...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([\"pip\", \"install\", \"datasets\", \"-q\"])\n",
        "            from datasets import load_dataset\n",
        "        \n",
        "        try:\n",
        "            # Load TinyStories dataset from Hugging Face\n",
        "            dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "            \n",
        "            # Limit number of stories if specified\n",
        "            if max_stories > 0 and len(dataset) > max_stories:\n",
        "                dataset = dataset.select(range(max_stories))\n",
        "            \n",
        "            print(f\"Processing {len(dataset):,} stories...\")\n",
        "            \n",
        "            # Combine stories into single text file\n",
        "            texts = []\n",
        "            for item in dataset:\n",
        "                text = item.get(\"text\", \"\")\n",
        "                if text:\n",
        "                    texts.append(text.strip())\n",
        "            \n",
        "            full_text = \"\\n\\n\".join(texts)\n",
        "            \n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(full_text)\n",
        "            \n",
        "            print(f\"Saved {len(full_text):,} characters to {file_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading TinyStories: {e}\")\n",
        "            print(\"Falling back to TinyShakespeare...\")\n",
        "            return download_tiny_shakespeare(data_dir)\n",
        "    else:\n",
        "        print(f\"Dataset already exists at {file_path}\")\n",
        "    \n",
        "    return file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_tiny_shakespeare(data_dir: str = \"data/raw\") -> str:\n",
        "    \"\"\"\n",
        "    Download the TinyShakespeare dataset (fallback/alternative).\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory to save the file\n",
        "        \n",
        "    Returns:\n",
        "        Path to the downloaded file\n",
        "    \"\"\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    file_path = os.path.join(data_dir, \"tiny_shakespeare.txt\")\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Downloading TinyShakespeare dataset...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(TINY_SHAKESPEARE_URL, file_path)\n",
        "            print(f\"Downloaded to {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading dataset: {e}\")\n",
        "            print(\"Please download manually from:\")\n",
        "            print(TINY_SHAKESPEARE_URL)\n",
        "            raise\n",
        "    else:\n",
        "        print(f\"Dataset already exists at {file_path}\")\n",
        "    \n",
        "    return file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    data_path: Optional[str] = None,\n",
        "    dataset_name: str = \"tinystories\",\n",
        "    seq_len: int = 128,\n",
        "    batch_size: int = 16,\n",
        "    train_split: float = 0.9,\n",
        "    num_workers: int = 0,\n",
        "    seed: int = 42,\n",
        "    max_stories: int = 50000\n",
        ") -> Tuple[DataLoader, DataLoader, CharTokenizer]:\n",
        "    \"\"\"\n",
        "    Create train and validation dataloaders.\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to text file (downloads dataset if None)\n",
        "        dataset_name: Dataset to use (\"tinystories\" or \"shakespeare\")\n",
        "        seq_len: Sequence length for samples\n",
        "        batch_size: Batch size\n",
        "        train_split: Fraction of data for training\n",
        "        num_workers: Number of data loading workers\n",
        "        seed: Random seed for reproducibility\n",
        "        max_stories: Max stories for TinyStories (default: 50000)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (train_loader, val_loader, tokenizer)\n",
        "        \n",
        "    Example:\n",
        "        >>> train_loader, val_loader, tokenizer = create_dataloaders(batch_size=32)\n",
        "        >>> for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        ...     print(x.shape, y.shape)  # (32, 128), (32, 128)\n",
        "        ...     break\n",
        "    \"\"\"\n",
        "    # Download dataset if needed\n",
        "    if data_path is None:\n",
        "        if dataset_name.lower() == \"tinystories\":\n",
        "            data_path = download_tinystories(max_stories=max_stories)\n",
        "        else:\n",
        "            data_path = download_tiny_shakespeare()\n",
        "    \n",
        "    # Load text\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    \n",
        "    print(f\"Loaded {len(text):,} characters\")\n",
        "    \n",
        "    # Create tokenizer\n",
        "    tokenizer = CharTokenizer(text)\n",
        "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = TextDataset(text, tokenizer, seq_len)\n",
        "    \n",
        "    # Split into train/val\n",
        "    n_train = int(len(dataset) * train_split)\n",
        "    n_val = len(dataset) - n_train\n",
        "    \n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, [n_train, n_val], generator=generator\n",
        "    )\n",
        "    \n",
        "    print(f\"Train samples: {len(train_dataset):,}, Val samples: {len(val_dataset):,}\")\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True  # Drop incomplete batches for consistent batch size\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "    \n",
        "    return train_loader, val_loader, tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Data Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing pipeline...\")\n",
        "    # NOTE: Set data_path to None to download, or point to a local file\n",
        "    try:\n",
        "        train_loader, val_loader, tokenizer = create_dataloaders(\n",
        "            batch_size=4, seq_len=32, max_stories=100\n",
        "        )\n",
        "        x, y = next(iter(train_loader))\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        print(f\"Target shape: {y.shape}\")\n",
        "        print(f\"Decoded: {tokenizer.decode(x[0].tolist())[:50]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not run test (missing internet?): {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
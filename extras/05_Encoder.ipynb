{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "Building the Encoder Block and the full Encoder Stack.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "# Note: Imports are simplified for standalone generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer Encoder Block.\n",
        "    \n",
        "    Implements the standard encoder layer with:\n",
        "    - Multi-head self-attention with residual connection\n",
        "    - Feed-forward network with residual connection\n",
        "    - Layer normalization (pre-norm or post-norm)\n",
        "    \n",
        "    Args:\n",
        "        d_model: Dimension of the model\n",
        "        n_heads: Number of attention heads\n",
        "        d_ff: Dimension of the feed-forward hidden layer\n",
        "        dropout: Dropout probability\n",
        "        activation: Activation function for FFN (\"gelu\" or \"relu\")\n",
        "        pre_norm: Whether to use pre-layer normalization (more stable)\n",
        "        \n",
        "    Example:\n",
        "        >>> block = EncoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
        "        >>> x = torch.randn(2, 10, 512)  # (batch, seq, d_model)\n",
        "        >>> output, attn = block(x)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        d_ff: int,\n",
        "        dropout: float = 0.1,\n",
        "        activation: str = \"gelu\",\n",
        "        pre_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-6\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.pre_norm = pre_norm\n",
        "        \n",
        "        # Multi-head self-attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout, activation)\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        \n",
        "        # Dropout for residual connections\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder block.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq, d_model)\n",
        "            mask: Optional attention mask for padding\n",
        "            return_attention: Whether to return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Output tensor of shape (batch, seq, d_model)\n",
        "                - Attention weights if return_attention=True, else None\n",
        "        \"\"\"\n",
        "        if self.pre_norm:\n",
        "            # Pre-Layer Normalization (more stable training)\n",
        "            # LN -> Attention -> Residual\n",
        "            attn_output, attn_weights = self._self_attention_block(\n",
        "                self.norm1(x), mask, return_attention\n",
        "            )\n",
        "            x = x + attn_output\n",
        "            \n",
        "            # LN -> FFN -> Residual\n",
        "            x = x + self._ff_block(self.norm2(x))\n",
        "        else:\n",
        "            # Post-Layer Normalization (original Transformer)\n",
        "            # Attention -> Residual -> LN\n",
        "            attn_output, attn_weights = self._self_attention_block(\n",
        "                x, mask, return_attention\n",
        "            )\n",
        "            x = self.norm1(x + attn_output)\n",
        "            \n",
        "            # FFN -> Residual -> LN\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "            \n",
        "        return x, attn_weights\n",
        "    \n",
        "    def _self_attention_block(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        return_attention: bool\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"Apply self-attention with dropout.\"\"\"\n",
        "        attn_output, attn_weights = self.self_attention(\n",
        "            x, x, x, mask=mask, return_attention=return_attention\n",
        "        )\n",
        "        return self.dropout(attn_output), attn_weights\n",
        "    \n",
        "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply feed-forward network with dropout.\"\"\"\n",
        "        return self.dropout(self.feed_forward(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Stack.\n",
        "    \n",
        "    Combines embedding layer with multiple encoder blocks to create\n",
        "    the complete encoder for processing input sequences.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        d_model: Dimension of the model\n",
        "        n_heads: Number of attention heads\n",
        "        n_layers: Number of encoder blocks\n",
        "        d_ff: Dimension of the feed-forward hidden layer\n",
        "        max_seq_len: Maximum sequence length\n",
        "        dropout: Dropout probability\n",
        "        activation: Activation function for FFN\n",
        "        pre_norm: Whether to use pre-layer normalization\n",
        "        \n",
        "    Example:\n",
        "        >>> encoder = Encoder(vocab_size=10000, d_model=512, n_heads=8, n_layers=6, d_ff=2048)\n",
        "        >>> tokens = torch.randint(0, 10000, (2, 50))  # (batch, seq)\n",
        "        >>> output, attentions = encoder(tokens, return_attention=True)\n",
        "        >>> print(output.shape)  # (2, 50, 512)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        n_layers: int,\n",
        "        d_ff: int,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        activation: str = \"gelu\",\n",
        "        pre_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-6\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # Embedding layer (token + positional)\n",
        "        self.embedding = TransformerEmbedding(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Stack of encoder blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                dropout=dropout,\n",
        "                activation=activation,\n",
        "                pre_norm=pre_norm,\n",
        "                layer_norm_eps=layer_norm_eps\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer normalization (only for pre-norm)\n",
        "        self.final_norm = nn.LayerNorm(d_model, eps=layer_norm_eps) if pre_norm else None\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        return_attention: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Encode input sequence.\n",
        "        \n",
        "        Args:\n",
        "            src: Source token indices of shape (batch, seq)\n",
        "            mask: Optional padding mask of shape (batch, 1, 1, seq)\n",
        "                 True values indicate positions to mask (not attend to)\n",
        "            return_attention: Whether to return attention weights from all layers\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "                - Encoded representation of shape (batch, seq, d_model)\n",
        "                - List of attention weights from each layer (if return_attention=True)\n",
        "        \"\"\"\n",
        "        attention_weights = [] if return_attention else None\n",
        "        \n",
        "        # Apply embeddings\n",
        "        x = self.embedding(src)\n",
        "        \n",
        "        # Pass through encoder blocks\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, mask=mask, return_attention=return_attention)\n",
        "            if return_attention and attn is not None:\n",
        "                attention_weights.append(attn)\n",
        "        \n",
        "        # Apply final layer normalization\n",
        "        if self.final_norm is not None:\n",
        "            x = self.final_norm(x)\n",
        "            \n",
        "        return x, attention_weights\n",
        "    \n",
        "    def get_embedding(self, src: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get just the embeddings without passing through layers.\"\"\"\n",
        "        return self.embedding(src)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}